# ============================================
# LLM API Configuration (Provider Agnostic)
# ============================================

# Required: Your LLM API key
# Supports: OpenAI, Hugging Face, Azure OpenAI, or any OpenAI-compatible API
# OpenAI: Get from https://platform.openai.com/api-keys (starts with sk-proj-...)
# Hugging Face: Get from https://huggingface.co/settings/tokens (starts with hf_...)
OPENAI_API_KEY=your-api-key-here

# Optional: Custom base URL for your LLM provider
# Default: https://api.openai.com/v1
#
# Provider Examples:
# - OpenAI (default): https://api.openai.com/v1
# - Hugging Face Inference: https://api-inference.huggingface.co/v1
# - Azure OpenAI: https://YOUR_RESOURCE.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT
# - Together.ai: https://api.together.xyz/v1
# - Anyscale: https://api.endpoints.anyscale.com/v1
# - LiteLLM Proxy: http://localhost:4000
OPENAI_BASE_URL=https://api.openai.com/v1

# Optional: Model to use for text enhancement
# Default: gpt-4o-mini
#
# Model Examples by Provider:
# - OpenAI: gpt-4o-mini, gpt-4o, gpt-4-turbo, gpt-3.5-turbo
# - Hugging Face: meta-llama/Meta-Llama-3.1-70B-Instruct, mistralai/Mixtral-8x7B-Instruct-v0.1
# - Azure OpenAI: Use your deployment name
# - Together.ai: meta-llama/Llama-3-70b-chat-hf, mistralai/Mixtral-8x7B-Instruct-v0.1
OPENAI_MODEL=gpt-4o-mini

# Optional: Model to use for image analysis (Vision)
# Default: gpt-4o
#
# Vision Model Examples:
# - OpenAI: gpt-4o, gpt-4-turbo, gpt-4-vision-preview
# - Hugging Face: llava-hf/llava-v1.6-mistral-7b-hf
# - Azure OpenAI: Use your vision deployment name
VISION_MODEL=gpt-4o

# Optional: Model to use for image generation
# Default: dall-e-3
#
# Image Generation Examples:
# - OpenAI: dall-e-3, dall-e-2
# - Hugging Face: stabilityai/stable-diffusion-xl-base-1.0, black-forest-labs/FLUX.1-dev
# - Replicate: stability-ai/sdxl
IMAGE_MODEL=dall-e-3

# Optional: Number of images to generate per request
# Default: 2
# Range: 1-10 (depends on provider and model)
# Note: DALL-E 3 only supports 1, generated sequentially
IMAGE_COUNT=2

# Optional: Image size for generation
# Default: 1024x1024
# DALL-E 3 supports: 1024x1024, 1792x1024, 1024x1792
# DALL-E 2 supports: 256x256, 512x512, 1024x1024
IMAGE_SIZE=1024x1024

# Optional: Image quality
# Default: standard
# Options: standard, hd (DALL-E 3 only, hd costs 2x)
IMAGE_QUALITY=standard

# ============================================
# Hugging Face Specific Configuration
# ============================================

# If using Hugging Face Inference API, set this to true
# This enables Hugging Face-specific error handling and retry logic
USE_HUGGINGFACE=false

# ============================================
# Development Notes
# ============================================

# 1. Copy this file to .env.local for local development:
#    cp .env.example .env.local
#
# 2. Never commit .env.local to version control (.gitignore blocks it)
#
# 3. For Vercel deployment, set these in the Vercel Dashboard:
#    Project Settings â†’ Environment Variables
#
# 4. Cost estimates (as of 2024):
#    OpenAI:
#      - GPT-4o-mini: ~$0.15/1M input tokens, ~$0.60/1M output tokens
#      - DALL-E 3 (standard, 1024x1024): $0.040 per image
#      - DALL-E 3 (hd, 1024x1024): $0.080 per image
#
#    Hugging Face Inference API:
#      - Free tier: Rate limited
#      - PRO ($9/month): Higher rate limits
#      - Enterprise: Custom pricing
#
# 5. Rate limits (OpenAI default tier):
#    - GPT-4o-mini: 30,000 TPM (tokens per minute)
#    - DALL-E 3: 5 images/minute, 100 images/day
#
# 6. Provider-specific setup:
#
#    For Hugging Face:
#      OPENAI_API_KEY=hf_xxxxxxxxxxxxxxxxxxxxx
#      OPENAI_BASE_URL=https://api-inference.huggingface.co/v1
#      OPENAI_MODEL=meta-llama/Meta-Llama-3.1-70B-Instruct
#      VISION_MODEL=llava-hf/llava-v1.6-mistral-7b-hf
#      USE_HUGGINGFACE=true
#
#    For Azure OpenAI:
#      OPENAI_API_KEY=your-azure-api-key
#      OPENAI_BASE_URL=https://YOUR_RESOURCE.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT
#      OPENAI_MODEL=your-deployment-name
